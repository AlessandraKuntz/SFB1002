{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62b62cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output structure created at: C:\\Users\\Kuntz1\\Desktop\\SFB1002_Analysis\n",
      "🚀 Starting SFB1002 Complete Publication Analysis...\n",
      "============================================================\n",
      "📊 Loading and cleaning data...\n",
      "📋 Loaded 550 rows and 142 columns\n",
      "🔍 First 10 column names:\n",
      "   0: RDP - Link\n",
      "   1: Working Groups\n",
      "   2: Subproject\n",
      "   3: Open Access\n",
      "   4: Publication Type\n",
      "   5: Peer Reviewed\n",
      "   6: PMID\n",
      "   7: DOI\n",
      "   8: Publication Year\n",
      "   9: Title\n",
      "🧹 Starting data cleaning...\n",
      "   Original columns: ['RDP - Link', 'Working Groups', 'Subproject', 'Open Access', 'Publication Type', 'Peer Reviewed', 'PMID', 'DOI', 'Publication Year', 'Title', 'Journal', 'ISSN', 'eISSN', 'URL', 'Pages', 'Issue', 'Volume', 'Journal Abbreviation', 'Extra', 'Authors', 'First Author', 'Last Author', 'Creation Time']\n",
      "   Cleaned columns: ['RDP_Link', 'Working_Groups', 'Subproject', 'Open_Access', 'Publication_Type', 'Peer_Reviewed', 'PMID', 'DOI', 'Publication_Year', 'Title']...\n",
      "   Filtered to years 2014-2024: 498 publications\n",
      "✅ Data cleaning completed: 498 publications ready for analysis\n",
      "📊 Creating specialized datasets...\n",
      "   ✅ Individual groups dataset: 984 rows\n",
      "   ✅ Collaboration dataset: 226 rows\n",
      "👥 Creating author dataset...\n",
      "   ✅ Author dataset: 5011 rows\n",
      "💾 Saved: main_dataset.csv (498 rows)\n",
      "💾 Saved: individual_groups.csv (984 rows)\n",
      "💾 Saved: collaboration_dataset.csv (226 rows)\n",
      "💾 Saved: author_dataset.csv (5011 rows)\n",
      "💾 Saved: external_resources.csv (550 rows)\n",
      "✅ Data cleaned: 498 publications from 2014-2024\n",
      "📈 Generating basic statistics...\n",
      "   ✅ Statistics saved to: C:\\Users\\Kuntz1\\Desktop\\SFB1002_Analysis\\04_statistics\\basic_statistics.txt\n",
      "⏰ Creating temporal analysis...\n",
      "🕸️ Creating collaboration networks...\n",
      "   ✅ Group network created with 36 nodes and 235 edges\n",
      "👥 Creating author network...\n",
      "🔬 Creating project collaboration network...\n",
      "   ⚠️ Error creating project network: cannot unpack non-iterable int object\n",
      "📅 Creating temporal collaboration networks...\n",
      "   ✅ Created network for 2014-2016: 28 nodes, 81 edges\n",
      "   ✅ Created network for 2017-2019: 31 nodes, 133 edges\n",
      "   ✅ Created network for 2020-2022: 32 nodes, 130 edges\n",
      "   ✅ Created network for 2023-2024: 22 nodes, 73 edges\n",
      "🎨 Creating comprehensive visualizations...\n",
      "📊 Creating productivity analysis...\n",
      "🎯 Generating strategic insights...\n",
      "\n",
      "============================================================\n",
      "✅ Analysis Complete!\n",
      "📁 All outputs saved to: C:\\Users\\Kuntz1\\Desktop\\SFB1002_Analysis\n",
      "\n",
      "📋 Summary:\n",
      "   • 498 publications analyzed\n",
      "   • 37 working groups\n",
      "   • 2359 unique authors\n",
      "   • 45.4% collaboration rate\n",
      "   • 10 organized output folders with 50+ analysis files\n",
      "\n",
      "🎯 Key files to review:\n",
      "   • 03_reports/executive_summary.html\n",
      "   • 02_visualizations/interactive/ (for detailed exploration)\n",
      "   • 06_collaboration_metrics/ (for network insights)\n",
      "\n",
      "🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
      "ANALYSIS COMPLETED SUCCESSFULLY!\n",
      "🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Charting Collaboration and Impact: \n",
    "A Data Analysis of SFB1002’s Research Output\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for static plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class SFB1002Analyzer:\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"Initialize the analyzer with the Excel file\"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.output_dir = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"SFB1002_Analysis\")\n",
    "        self.create_output_structure()\n",
    "        \n",
    "    def create_output_structure(self):\n",
    "        \"\"\"Create organized output folder structure\"\"\"\n",
    "        directories = [\n",
    "            \"01_cleaned_data\",\n",
    "            \"02_visualizations/static\",\n",
    "            \"02_visualizations/interactive\", \n",
    "            \"02_visualizations/networks\",\n",
    "            \"03_reports\",\n",
    "            \"04_statistics\",\n",
    "            \"05_temporal_analysis\",\n",
    "            \"06_collaboration_metrics\",\n",
    "            \"07_productivity_analysis\",\n",
    "            \"08_impact_analysis\",\n",
    "            \"09_strategic_insights\"\n",
    "        ]\n",
    "        \n",
    "        for directory in directories:\n",
    "            path = os.path.join(self.output_dir, directory)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            \n",
    "        print(f\"✅ Output structure created at: {self.output_dir}\")\n",
    "    \n",
    "    def load_and_clean_data(self):\n",
    "        \"\"\"Load Excel file and perform comprehensive data cleaning\"\"\"\n",
    "        print(\"📊 Loading and cleaning data...\")\n",
    "        \n",
    "        # Load the Excel file\n",
    "        df = pd.read_excel(self.file_path)\n",
    "        \n",
    "        print(f\"📋 Loaded {len(df)} rows and {len(df.columns)} columns\")\n",
    "        print(\"🔍 First 10 column names:\")\n",
    "        for i, col in enumerate(df.columns[:10]):\n",
    "            print(f\"   {i}: {col}\")\n",
    "        \n",
    "        # Identify core columns (first 23 with actual data)\n",
    "        core_columns = df.columns[:23].tolist()\n",
    "        external_columns = df.columns[23:].tolist()\n",
    "        \n",
    "        # Create main dataset with core columns\n",
    "        self.main_df = df[core_columns].copy()\n",
    "        \n",
    "        # Create external resources dataset\n",
    "        self.external_df = df[external_columns].copy()\n",
    "        \n",
    "        # Clean main dataset\n",
    "        self.clean_main_dataset()\n",
    "        \n",
    "        # Create specialized datasets\n",
    "        self.create_specialized_datasets()\n",
    "        \n",
    "        # Save cleaned datasets\n",
    "        self.save_cleaned_datasets()\n",
    "        \n",
    "        print(f\"✅ Data cleaned: {len(self.main_df)} publications from {self.main_df['Publication_Year'].min()}-{self.main_df['Publication_Year'].max()}\")\n",
    "    \n",
    "    def clean_main_dataset(self):\n",
    "        \"\"\"Clean the main dataset\"\"\"\n",
    "        df = self.main_df\n",
    "        \n",
    "        print(\"🧹 Starting data cleaning...\")\n",
    "        print(f\"   Original columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Create a mapping of original to clean column names\n",
    "        # Handle the exact column names from your Excel file\n",
    "        expected_columns = [\n",
    "            'RDP - Link', 'Working Groups', 'Subproject', 'Open Access', 'Publication Type',\n",
    "            'Peer Reviewed', 'PMID', 'DOI', 'Publication Year', 'Title', 'Journal',\n",
    "            'ISSN', 'eISSN', 'URL', 'Pages', 'Issue', 'Volume', 'Journal Abbreviation',\n",
    "            'Extra', 'Authors', 'First Author', 'Last Author', 'Creation Time'\n",
    "        ]\n",
    "        \n",
    "        # Map actual columns to expected columns (handle slight differences)\n",
    "        column_mapping = {}\n",
    "        for i, actual_col in enumerate(df.columns[:len(expected_columns)]):\n",
    "            if i < len(expected_columns):\n",
    "                column_mapping[actual_col] = expected_columns[i]\n",
    "        \n",
    "        # Rename columns\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Create clean column names for processing\n",
    "        clean_column_names = [\n",
    "            'RDP_Link', 'Working_Groups', 'Subproject', 'Open_Access', 'Publication_Type',\n",
    "            'Peer_Reviewed', 'PMID', 'DOI', 'Publication_Year', 'Title', 'Journal',\n",
    "            'ISSN', 'eISSN', 'URL', 'Pages', 'Issue', 'Volume', 'Journal_Abbreviation',\n",
    "            'Extra', 'Authors', 'First_Author', 'Last_Author', 'Creation_Time'\n",
    "        ]\n",
    "        \n",
    "        # Create another mapping for clean names\n",
    "        final_mapping = {}\n",
    "        for i, col in enumerate(df.columns[:len(clean_column_names)]):\n",
    "            if i < len(clean_column_names):\n",
    "                final_mapping[col] = clean_column_names[i]\n",
    "        \n",
    "        df = df.rename(columns=final_mapping)\n",
    "        \n",
    "        print(f\"   Cleaned columns: {list(df.columns[:10])}...\")\n",
    "        \n",
    "        # Clean publication year\n",
    "        if 'Publication_Year' in df.columns:\n",
    "            df['Publication_Year'] = pd.to_numeric(df['Publication_Year'], errors='coerce')\n",
    "            df = df.dropna(subset=['Publication_Year'])\n",
    "            df['Publication_Year'] = df['Publication_Year'].astype(int)\n",
    "            \n",
    "            # Filter to complete years (2014-2024)\n",
    "            df = df[(df['Publication_Year'] >= 2014) & (df['Publication_Year'] <= 2024)]\n",
    "            print(f\"   Filtered to years 2014-2024: {len(df)} publications\")\n",
    "        else:\n",
    "            print(\"   ⚠️ Warning: Publication_Year column not found!\")\n",
    "            return\n",
    "        \n",
    "        # Clean Working Groups\n",
    "        if 'Working_Groups' in df.columns:\n",
    "            df['Working_Groups'] = df['Working_Groups'].fillna('Unknown')\n",
    "            df['Working_Groups_Clean'] = df['Working_Groups'].str.replace('ag_', '', regex=False)\n",
    "        \n",
    "        # Clean Publication Type\n",
    "        if 'Publication_Type' in df.columns:\n",
    "            df['Publication_Type'] = df['Publication_Type'].fillna('Unknown')\n",
    "        \n",
    "        # Clean Open Access\n",
    "        if 'Open_Access' in df.columns:\n",
    "            df['Open_Access_Binary'] = df['Open_Access'].map({'Yes': 1, 'No': 0})\n",
    "            df['Open_Access_Binary'] = df['Open_Access_Binary'].fillna(0)\n",
    "        \n",
    "        # Add derived columns\n",
    "        df['Year_Period'] = pd.cut(df['Publication_Year'], \n",
    "                                 bins=[2013, 2016, 2019, 2022, 2024], \n",
    "                                 labels=['2014-2016', '2017-2019', '2020-2022', '2023-2024'])\n",
    "        \n",
    "        # Extract number of collaborating groups\n",
    "        if 'Working_Groups' in df.columns:\n",
    "            df['Num_Collaborating_Groups'] = df['Working_Groups'].str.count(',') + 1\n",
    "            df['Is_Collaboration'] = df['Num_Collaborating_Groups'] > 1\n",
    "        \n",
    "        # Clean journal information\n",
    "        if 'Journal' in df.columns:\n",
    "            df['Journal'] = df['Journal'].fillna('Unknown Journal')\n",
    "        \n",
    "        self.main_df = df\n",
    "        print(f\"✅ Data cleaning completed: {len(df)} publications ready for analysis\")\n",
    "    \n",
    "    def create_specialized_datasets(self):\n",
    "        \"\"\"Create specialized datasets for different analysis perspectives\"\"\"\n",
    "        \n",
    "        print(\"📊 Creating specialized datasets...\")\n",
    "        \n",
    "        # Check required columns exist\n",
    "        required_cols = ['Working_Groups', 'Subproject', 'Authors']\n",
    "        missing_cols = [col for col in required_cols if col not in self.main_df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"   ⚠️ Warning: Missing columns {missing_cols}\")\n",
    "            print(f\"   Available columns: {list(self.main_df.columns)}\")\n",
    "        \n",
    "        # Dataset A: Individual groups (split multi-group publications)\n",
    "        individual_rows = []\n",
    "        for _, row in self.main_df.iterrows():\n",
    "            if pd.notna(row['Working_Groups']):\n",
    "                groups = [g.strip() for g in str(row['Working_Groups']).split(',')]\n",
    "                subprojects = [s.strip() for s in str(row.get('Subproject', '')).split(',')]\n",
    "                \n",
    "                # Ensure equal length arrays\n",
    "                max_len = max(len(groups), len(subprojects))\n",
    "                groups.extend([''] * (max_len - len(groups)))\n",
    "                subprojects.extend([''] * (max_len - len(subprojects)))\n",
    "                \n",
    "                for group, subproject in zip(groups, subprojects):\n",
    "                    new_row = row.copy()\n",
    "                    new_row['Working_Groups'] = group\n",
    "                    new_row['Working_Groups_Clean'] = group.replace('ag_', '')\n",
    "                    new_row['Subproject'] = subproject\n",
    "                    individual_rows.append(new_row)\n",
    "        \n",
    "        self.individual_df = pd.DataFrame(individual_rows)\n",
    "        print(f\"   ✅ Individual groups dataset: {len(self.individual_df)} rows\")\n",
    "        \n",
    "        # Dataset B: Collaboration-focused (keep combined groups)\n",
    "        if 'Is_Collaboration' in self.main_df.columns:\n",
    "            self.collaboration_df = self.main_df[self.main_df['Is_Collaboration'] == True].copy()\n",
    "        else:\n",
    "            self.collaboration_df = pd.DataFrame()\n",
    "        print(f\"   ✅ Collaboration dataset: {len(self.collaboration_df)} rows\")\n",
    "        \n",
    "        # Author-based dataset\n",
    "        self.create_author_dataset()\n",
    "    \n",
    "    def create_author_dataset(self):\n",
    "        \"\"\"Create author-based dataset for person-level network analysis\"\"\"\n",
    "        print(\"👥 Creating author dataset...\")\n",
    "        \n",
    "        author_rows = []\n",
    "        \n",
    "        if 'Authors' not in self.main_df.columns:\n",
    "            print(\"   ⚠️ Warning: 'Authors' column not found, creating empty author dataset\")\n",
    "            self.author_df = pd.DataFrame()\n",
    "            return\n",
    "        \n",
    "        for _, row in self.main_df.iterrows():\n",
    "            if pd.notna(row['Authors']):\n",
    "                # Split authors by comma\n",
    "                authors = [a.strip() for a in str(row['Authors']).split(',')]\n",
    "                \n",
    "                for author in authors:\n",
    "                    if author and len(author) > 2:  # Filter out initials only\n",
    "                        new_row = row.copy()\n",
    "                        new_row['Author_Name'] = author\n",
    "                        \n",
    "                        # Check if first/last author columns exist\n",
    "                        if 'First_Author' in row.index:\n",
    "                            new_row['Is_First_Author'] = (author == row['First_Author'])\n",
    "                        else:\n",
    "                            new_row['Is_First_Author'] = False\n",
    "                            \n",
    "                        if 'Last_Author' in row.index:\n",
    "                            new_row['Is_Last_Author'] = (author == row['Last_Author'])\n",
    "                        else:\n",
    "                            new_row['Is_Last_Author'] = False\n",
    "                            \n",
    "                        author_rows.append(new_row)\n",
    "        \n",
    "        self.author_df = pd.DataFrame(author_rows)\n",
    "        print(f\"   ✅ Author dataset: {len(self.author_df)} rows\")\n",
    "    \n",
    "    def save_cleaned_datasets(self):\n",
    "        \"\"\"Save all cleaned datasets\"\"\"\n",
    "        datasets = {\n",
    "            'main_dataset.csv': self.main_df,\n",
    "            'individual_groups.csv': self.individual_df,\n",
    "            'collaboration_dataset.csv': self.collaboration_df,\n",
    "            'author_dataset.csv': self.author_df,\n",
    "            'external_resources.csv': self.external_df\n",
    "        }\n",
    "        \n",
    "        for filename, df in datasets.items():\n",
    "            path = os.path.join(self.output_dir, \"01_cleaned_data\", filename)\n",
    "            df.to_csv(path, index=False)\n",
    "            print(f\"💾 Saved: {filename} ({len(df)} rows)\")\n",
    "    \n",
    "    def generate_basic_statistics(self):\n",
    "        \"\"\"Generate comprehensive basic statistics\"\"\"\n",
    "        print(\"📈 Generating basic statistics...\")\n",
    "        \n",
    "        try:\n",
    "            # Check if we have the required data\n",
    "            if len(self.main_df) == 0:\n",
    "                print(\"   ⚠️ Warning: No data available for statistics\")\n",
    "                return {}\n",
    "            \n",
    "            # Safe statistics generation with error handling\n",
    "            stats = {\n",
    "                'Overview': {\n",
    "                    'Total Publications': len(self.main_df),\n",
    "                    'Unique Working Groups': self.individual_df['Working_Groups'].nunique() if len(self.individual_df) > 0 else 0,\n",
    "                    'Unique Authors': self.author_df['Author_Name'].nunique() if len(self.author_df) > 0 else 0,\n",
    "                    'Unique Journals': self.main_df['Journal'].nunique() if 'Journal' in self.main_df.columns else 0,\n",
    "                    'Year Range': f\"{self.main_df['Publication_Year'].min()}-{self.main_df['Publication_Year'].max()}\" if 'Publication_Year' in self.main_df.columns else 'Unknown',\n",
    "                    'Collaboration Rate': f\"{(self.main_df['Is_Collaboration'].sum() / len(self.main_df) * 100):.1f}%\" if 'Is_Collaboration' in self.main_df.columns else 'N/A',\n",
    "                    'Open Access Rate': f\"{(self.main_df['Open_Access_Binary'].sum() / len(self.main_df) * 100):.1f}%\" if 'Open_Access_Binary' in self.main_df.columns else 'N/A'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add additional statistics only if data is available\n",
    "            if 'Publication_Type' in self.main_df.columns:\n",
    "                stats['Publication Types'] = dict(self.main_df['Publication_Type'].value_counts())\n",
    "                \n",
    "            if len(self.individual_df) > 0 and 'Working_Groups_Clean' in self.individual_df.columns:\n",
    "                stats['Top Working Groups'] = dict(self.individual_df['Working_Groups_Clean'].value_counts().head(10))\n",
    "                \n",
    "            if 'Journal' in self.main_df.columns:\n",
    "                stats['Top Journals'] = dict(self.main_df['Journal'].value_counts().head(10))\n",
    "                \n",
    "            if len(self.author_df) > 0 and 'Author_Name' in self.author_df.columns:\n",
    "                stats['Top Authors'] = dict(self.author_df['Author_Name'].value_counts().head(15))\n",
    "                \n",
    "            if 'Publication_Year' in self.main_df.columns:\n",
    "                stats['Yearly Publications'] = dict(self.main_df['Publication_Year'].value_counts().sort_index())\n",
    "            \n",
    "            # Save statistics\n",
    "            stats_path = os.path.join(self.output_dir, \"04_statistics\", \"basic_statistics.txt\")\n",
    "            with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "                for section, data in stats.items():\n",
    "                    f.write(f\"\\n{'='*50}\\n{section.upper()}\\n{'='*50}\\n\")\n",
    "                    if isinstance(data, dict):\n",
    "                        for key, value in data.items():\n",
    "                            f.write(f\"{key}: {value}\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"{data}\\n\")\n",
    "            \n",
    "            print(f\"   ✅ Statistics saved to: {stats_path}\")\n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Error generating statistics: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def create_temporal_analysis(self):\n",
    "        \"\"\"Comprehensive temporal analysis\"\"\"\n",
    "        print(\"⏰ Creating temporal analysis...\")\n",
    "        \n",
    "        # 1. Publications over time by group\n",
    "        yearly_groups = self.individual_df.groupby(['Publication_Year', 'Working_Groups_Clean']).size().unstack(fill_value=0)\n",
    "        \n",
    "        # Static plot\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        top_groups = self.individual_df['Working_Groups_Clean'].value_counts().head(10).index\n",
    "        yearly_groups[top_groups].plot(kind='line', marker='o', linewidth=2, markersize=6)\n",
    "        plt.title('Publication Trends: Top 10 Working Groups Over Time', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Year', fontsize=12)\n",
    "        plt.ylabel('Number of Publications', fontsize=12)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/static\", \"temporal_trends_groups.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Interactive temporal analysis\n",
    "        fig = px.line(\n",
    "            yearly_groups[top_groups].reset_index(), \n",
    "            x='Publication_Year', \n",
    "            y=top_groups.tolist(),\n",
    "            title='Interactive Publication Trends by Working Group',\n",
    "            labels={'value': 'Publications', 'variable': 'Working Group'}\n",
    "        )\n",
    "        fig.write_html(os.path.join(self.output_dir, \"02_visualizations/interactive\", \"temporal_trends_interactive.html\"))\n",
    "        \n",
    "        # 2. Collaboration evolution over time\n",
    "        collab_evolution = self.main_df.groupby(['Publication_Year', 'Is_Collaboration']).size().unstack(fill_value=0)\n",
    "        collab_evolution['Collaboration_Rate'] = collab_evolution[True] / (collab_evolution[True] + collab_evolution[False]) * 100\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        collab_evolution['Collaboration_Rate'].plot(kind='line', marker='o', linewidth=3, markersize=8, color='red')\n",
    "        plt.title('Collaboration Rate Evolution Over Time', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Year', fontsize=12)\n",
    "        plt.ylabel('Collaboration Rate (%)', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/static\", \"collaboration_evolution.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save temporal data\n",
    "        temporal_data = {\n",
    "            'yearly_publications_by_group': yearly_groups,\n",
    "            'collaboration_evolution': collab_evolution\n",
    "        }\n",
    "        \n",
    "        for name, data in temporal_data.items():\n",
    "            data.to_csv(os.path.join(self.output_dir, \"05_temporal_analysis\", f\"{name}.csv\"))\n",
    "    \n",
    "    def create_collaboration_networks(self):\n",
    "        \"\"\"Create comprehensive collaboration networks\"\"\"\n",
    "        print(\"🕸️ Creating collaboration networks...\")\n",
    "        \n",
    "        # 1. Working Group Collaboration Network\n",
    "        self.create_group_network()\n",
    "        \n",
    "        # 2. Author Collaboration Network\n",
    "        self.create_author_network()\n",
    "        \n",
    "        # 3. Cross-Project Collaboration Network\n",
    "        self.create_project_network()\n",
    "        \n",
    "        # 4. Temporal collaboration networks\n",
    "        self.create_temporal_networks()\n",
    "    \n",
    "    def create_group_network(self):\n",
    "        \"\"\"Create working group collaboration network\"\"\"\n",
    "        \n",
    "        try:\n",
    "            G = nx.Graph()\n",
    "            \n",
    "            if len(self.collaboration_df) == 0:\n",
    "                print(\"   ⚠️ No collaboration data available for group network\")\n",
    "                return G, {}\n",
    "            \n",
    "            # Add nodes and edges based on co-publications\n",
    "            for _, row in self.collaboration_df.iterrows():\n",
    "                if pd.notna(row.get('Working_Groups', '')):\n",
    "                    groups = [g.strip().replace('ag_', '') for g in str(row['Working_Groups']).split(',')]\n",
    "                    \n",
    "                    # Add nodes\n",
    "                    for group in groups:\n",
    "                        if group:  # Only add non-empty groups\n",
    "                            if G.has_node(group):\n",
    "                                G.nodes[group]['publications'] += 1\n",
    "                            else:\n",
    "                                G.add_node(group, publications=1)\n",
    "                    \n",
    "                    # Add edges (collaborations)\n",
    "                    for i in range(len(groups)):\n",
    "                        for j in range(i+1, len(groups)):\n",
    "                            if groups[i] and groups[j]:  # Only add edges for non-empty groups\n",
    "                                if G.has_edge(groups[i], groups[j]):\n",
    "                                    G[groups[i]][groups[j]]['weight'] += 1\n",
    "                                else:\n",
    "                                    G.add_edge(groups[i], groups[j], weight=1)\n",
    "            \n",
    "            if len(G.nodes()) == 0:\n",
    "                print(\"   ⚠️ No valid groups found for network\")\n",
    "                return G, {}\n",
    "            \n",
    "            # Calculate network metrics\n",
    "            centrality_metrics = {}\n",
    "            \n",
    "            try:\n",
    "                centrality_metrics['betweenness'] = nx.betweenness_centrality(G)\n",
    "                centrality_metrics['closeness'] = nx.closeness_centrality(G)\n",
    "                centrality_metrics['degree'] = nx.degree_centrality(G)\n",
    "                \n",
    "                # Only calculate eigenvector centrality if graph is connected\n",
    "                if nx.is_connected(G):\n",
    "                    centrality_metrics['eigenvector'] = nx.eigenvector_centrality(G)\n",
    "                else:\n",
    "                    # For disconnected graphs, calculate for largest component\n",
    "                    largest_cc = max(nx.connected_components(G), key=len)\n",
    "                    G_largest = G.subgraph(largest_cc)\n",
    "                    eig_centrality = nx.eigenvector_centrality(G_largest)\n",
    "                    # Fill in zeros for nodes not in largest component\n",
    "                    centrality_metrics['eigenvector'] = {node: eig_centrality.get(node, 0) for node in G.nodes()}\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error calculating centrality metrics: {str(e)}\")\n",
    "                centrality_metrics = {'degree': nx.degree_centrality(G)}\n",
    "            \n",
    "            # Save network metrics\n",
    "            if centrality_metrics:\n",
    "                metrics_df = pd.DataFrame(centrality_metrics)\n",
    "                metrics_df.to_csv(os.path.join(self.output_dir, \"06_collaboration_metrics\", \"group_centrality_metrics.csv\"))\n",
    "            \n",
    "            # Visualize network\n",
    "            plt.figure(figsize=(16, 12))\n",
    "            pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "            \n",
    "            # Node sizes based on publications\n",
    "            node_sizes = [max(G.nodes[node]['publications'] * 100, 100) for node in G.nodes()]\n",
    "            \n",
    "            # Edge weights\n",
    "            edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "            \n",
    "            nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', alpha=0.7)\n",
    "            if edge_weights:  # Only draw edges if they exist\n",
    "                nx.draw_networkx_edges(G, pos, width=[w*0.5 for w in edge_weights], alpha=0.5)\n",
    "            nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "            \n",
    "            plt.title('Working Group Collaboration Network\\n(Node size = Publications, Edge width = Collaborations)', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, \"02_visualizations/networks\", \"group_collaboration_network.png\"), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Save network\n",
    "            nx.write_gexf(G, os.path.join(self.output_dir, \"02_visualizations/networks\", \"group_network.gexf\"))\n",
    "            \n",
    "            print(f\"   ✅ Group network created with {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
    "            return G, centrality_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Error creating group network: {str(e)}\")\n",
    "            return nx.Graph(), {}\n",
    "    \n",
    "    def create_author_network(self):\n",
    "        \"\"\"Create author collaboration network\"\"\"\n",
    "        print(\"👥 Creating author network...\")\n",
    "        \n",
    "        # Focus on prolific authors (>= 3 publications)\n",
    "        author_counts = self.author_df['Author_Name'].value_counts()\n",
    "        prolific_authors = author_counts[author_counts >= 3].index\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Group authors by publication\n",
    "        pub_authors = defaultdict(list)\n",
    "        for _, row in self.author_df.iterrows():\n",
    "            if row['Author_Name'] in prolific_authors:\n",
    "                pub_id = f\"{row['DOI']}_{row['Publication_Year']}\"\n",
    "                pub_authors[pub_id].append(row['Author_Name'])\n",
    "        \n",
    "        # Create collaboration edges\n",
    "        for pub_id, authors in pub_authors.items():\n",
    "            for i in range(len(authors)):\n",
    "                for j in range(i+1, len(authors)):\n",
    "                    if G.has_edge(authors[i], authors[j]):\n",
    "                        G[authors[i]][authors[j]]['weight'] += 1\n",
    "                    else:\n",
    "                        G.add_edge(authors[i], authors[j], weight=1)\n",
    "        \n",
    "        # Add node attributes\n",
    "        for author in prolific_authors:\n",
    "            if author in G.nodes():\n",
    "                G.nodes[author]['publications'] = author_counts[author]\n",
    "        \n",
    "        # Network layout and visualization\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        node_sizes = [G.nodes[node].get('publications', 1) * 50 for node in G.nodes()]\n",
    "        edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightcoral', alpha=0.7)\n",
    "        nx.draw_networkx_edges(G, pos, width=[w*0.3 for w in edge_weights], alpha=0.4)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=6)\n",
    "        \n",
    "        plt.title('Author Collaboration Network\\n(Prolific Authors: ≥3 Publications)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/networks\", \"author_collaboration_network.png\"), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Get the 20 most connected authors based on degree centrality\n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "        top_20_authors = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "        top_20_nodes = [author for author, centrality in top_20_authors]\n",
    "\n",
    "        # Create subgraph with only top 20 authors and their connections\n",
    "        G_top20 = G.subgraph(top_20_nodes).copy()\n",
    "\n",
    "        # Network layout and visualization\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        pos = nx.spring_layout(G_top20, k=2, iterations=50)\n",
    "\n",
    "        node_sizes = [G_top20.nodes[node].get('publications', 1) * 100 for node in G_top20.nodes()]\n",
    "        edge_weights = [G_top20[u][v]['weight'] for u, v in G_top20.edges()]\n",
    "\n",
    "        nx.draw_networkx_nodes(G_top20, pos, node_size=node_sizes, node_color='lightcoral', alpha=0.7)\n",
    "        nx.draw_networkx_edges(G_top20, pos, width=[w*0.5 for w in edge_weights], alpha=0.4)\n",
    "        nx.draw_networkx_labels(G_top20, pos, font_size=8)\n",
    "\n",
    "        plt.title('Author Collaboration Network\\n(Top 20 Most Connected Authors)', \n",
    "                fontsize=16, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/networks\", \"top20_author_collaboration_network.png\"), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Calculate author centrality\n",
    "        if len(G.nodes()) > 0:\n",
    "            author_centrality = {\n",
    "                'betweenness': nx.betweenness_centrality(G),\n",
    "                'degree': nx.degree_centrality(G),\n",
    "                'closeness': nx.closeness_centrality(G)\n",
    "            }\n",
    "            \n",
    "            author_metrics_df = pd.DataFrame(author_centrality)\n",
    "            author_metrics_df.to_csv(os.path.join(self.output_dir, \"06_collaboration_metrics\", \"author_centrality_metrics.csv\"))\n",
    "        \n",
    "        nx.write_gexf(G, os.path.join(self.output_dir, \"02_visualizations/networks\", \"author_network.gexf\"))\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def create_project_network(self):\n",
    "        \"\"\"Create cross-project collaboration network\"\"\"\n",
    "        print(\"🔬 Creating project collaboration network...\")\n",
    "        \n",
    "        try:\n",
    "            # Clean subproject data\n",
    "            project_collabs = []\n",
    "            \n",
    "            if len(self.collaboration_df) == 0:\n",
    "                print(\"   ⚠️ No collaboration data available for project network\")\n",
    "                return nx.Graph()\n",
    "            \n",
    "            for _, row in self.collaboration_df.iterrows():\n",
    "                if pd.notna(row.get('Subproject', '')) and ',' in str(row.get('Subproject', '')):\n",
    "                    projects = [p.strip() for p in str(row['Subproject']).split(',')]\n",
    "                    projects = [p for p in projects if p and len(p) >= 2]  # Filter valid project codes\n",
    "                    if len(projects) > 1:\n",
    "                        for i in range(len(projects)):\n",
    "                            for j in range(i+1, len(projects)):\n",
    "                                project_collabs.append((projects[i], projects[j], row['Publication_Year']))\n",
    "            \n",
    "            if not project_collabs:\n",
    "                print(\"   ⚠️ No cross-project collaborations found\")\n",
    "                return nx.Graph()\n",
    "            \n",
    "            # Create network\n",
    "            G = nx.Graph()\n",
    "            for proj1, proj2, year in project_collabs:\n",
    "                if G.has_edge(proj1, proj2):\n",
    "                    G[proj1][proj2]['weight'] += 1\n",
    "                    G[proj1][proj2]['years'].append(year)\n",
    "                else:\n",
    "                    G.add_edge(proj1, proj2, weight=1, years=[year])\n",
    "            \n",
    "            if len(G.nodes()) > 0:\n",
    "                # Visualize\n",
    "                plt.figure(figsize=(14, 10))\n",
    "                pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "                \n",
    "                edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "                node_sizes = [G.degree(node) * 200 + 300 for node in G.nodes()]\n",
    "                \n",
    "                nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightgreen', alpha=0.8)\n",
    "                nx.draw_networkx_edges(G, pos, width=[w*2 for w in edge_weights], alpha=0.6)\n",
    "                nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "                \n",
    "                plt.title('Cross-Project Collaboration Network\\n(Subproject Collaborations)', \n",
    "                         fontsize=16, fontweight='bold')\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(self.output_dir, \"02_visualizations/networks\", \"project_collaboration_network.png\"), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                nx.write_gexf(G, os.path.join(self.output_dir, \"02_visualizations/networks\", \"project_network.gexf\"))\n",
    "                print(f\"   ✅ Project network created with {len(G.nodes())} nodes and {len(G.edges())} edges\")\n",
    "            else:\n",
    "                print(\"   ⚠️ No project network nodes to visualize\")\n",
    "            \n",
    "            return G\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Error creating project network: {str(e)}\")\n",
    "            return nx.Graph()\n",
    "    \n",
    "    def create_temporal_networks(self):\n",
    "        \"\"\"Create networks showing collaboration evolution\"\"\"\n",
    "        print(\"📅 Creating temporal collaboration networks...\")\n",
    "        \n",
    "        try:\n",
    "            periods = ['2014-2016', '2017-2019', '2020-2022', '2023-2024']\n",
    "            \n",
    "            for period in periods:\n",
    "                if 'Year_Period' not in self.main_df.columns:\n",
    "                    print(f\"   ⚠️ Year_Period column not found, skipping temporal networks\")\n",
    "                    return\n",
    "                \n",
    "                period_data = self.main_df[self.main_df['Year_Period'] == period]\n",
    "                \n",
    "                if len(period_data) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                period_collabs = period_data[period_data.get('Is_Collaboration', False) == True]\n",
    "                \n",
    "                if len(period_collabs) == 0:\n",
    "                    continue\n",
    "                \n",
    "                G = nx.Graph()\n",
    "                \n",
    "                for _, row in period_collabs.iterrows():\n",
    "                    if pd.notna(row.get('Working_Groups', '')):\n",
    "                        groups = [g.strip().replace('ag_', '') for g in str(row['Working_Groups']).split(',')]\n",
    "                        \n",
    "                        for i in range(len(groups)):\n",
    "                            for j in range(i+1, len(groups)):\n",
    "                                if G.has_edge(groups[i], groups[j]):\n",
    "                                    G[groups[i]][groups[j]]['weight'] += 1\n",
    "                                else:\n",
    "                                    G.add_edge(groups[i], groups[j], weight=1)\n",
    "                \n",
    "                if len(G.nodes()) > 0:\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "                    \n",
    "                    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "                    node_sizes = [G.degree(node) * 100 + 200 for node in G.nodes()]\n",
    "                    \n",
    "                    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='orange', alpha=0.7)\n",
    "                    nx.draw_networkx_edges(G, pos, width=[w for w in edge_weights], alpha=0.5)\n",
    "                    nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "                    \n",
    "                    plt.title(f'Working Group Collaborations: {period}', fontsize=14, fontweight='bold')\n",
    "                    plt.axis('off')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(self.output_dir, \"02_visualizations/networks\", f\"network_{period.replace('-', '_')}.png\"), \n",
    "                               dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    \n",
    "                    print(f\"   ✅ Created network for {period}: {len(G.nodes())} nodes, {len(G.edges())} edges\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ No collaboration data for {period}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Error creating temporal networks: {str(e)}\")\n",
    "    \n",
    "    def create_comprehensive_visualizations(self):\n",
    "        \"\"\"Create comprehensive static and interactive visualizations\"\"\"\n",
    "        print(\"🎨 Creating comprehensive visualizations...\")\n",
    "        \n",
    "        # 1. Publication timeline\n",
    "        self.create_publication_timeline()\n",
    "        \n",
    "        # 2. Working group activity\n",
    "        self.create_group_activity_charts()\n",
    "        \n",
    "        # 3. Collaboration analysis\n",
    "        self.create_collaboration_analysis()\n",
    "        \n",
    "        # 4. Publication type analysis\n",
    "        self.create_publication_type_analysis()\n",
    "        \n",
    "        # 5. Journal and impact analysis\n",
    "        self.create_journal_analysis()\n",
    "        \n",
    "        # 6. Open access analysis\n",
    "        self.create_open_access_analysis()\n",
    "    \n",
    "    def create_publication_timeline(self):\n",
    "        \"\"\"Create detailed publication timeline visualizations\"\"\"\n",
    "        \n",
    "        # Static timeline\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        yearly_counts = self.main_df['Publication_Year'].value_counts().sort_index()\n",
    "        \n",
    "        bars = plt.bar(yearly_counts.index, yearly_counts.values, color='skyblue', alpha=0.8, edgecolor='navy')\n",
    "        plt.plot(yearly_counts.index, yearly_counts.values, color='red', marker='o', linewidth=2, markersize=6)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.title('SFB1002 Publications Timeline (2014-2024)', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Year', fontsize=12)\n",
    "        plt.ylabel('Number of Publications', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/static\", \"publication_timeline.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Interactive timeline with additional metrics\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=('Publications per Year', 'Collaboration Rate per Year'),\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # Publications\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=yearly_counts.index, y=yearly_counts.values, mode='lines+markers', name='Publications'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Collaboration rate\n",
    "        collab_rate = self.main_df.groupby('Publication_Year')['Is_Collaboration'].mean() * 100\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=collab_rate.index, y=collab_rate.values, mode='lines+markers', \n",
    "                      name='Collaboration Rate (%)', line=dict(color='red')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=600, title_text=\"SFB1002 Publication Metrics Over Time\")\n",
    "        fig.write_html(os.path.join(self.output_dir, \"02_visualizations/interactive\", \"publication_timeline_interactive.html\"))\n",
    "    \n",
    "    def create_group_activity_charts(self):\n",
    "        \"\"\"Create working group activity analysis\"\"\"\n",
    "        \n",
    "        # Top 15 most active groups\n",
    "        top_groups = self.individual_df['Working_Groups_Clean'].value_counts().head(15)\n",
    "        \n",
    "        # Static horizontal bar chart\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        bars = plt.barh(range(len(top_groups)), top_groups.values, color='lightcoral')\n",
    "        plt.yticks(range(len(top_groups)), top_groups.index)\n",
    "        plt.xlabel('Number of Publications', fontsize=12)\n",
    "        plt.title('Top 15 Most Active Working Groups', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, value) in enumerate(zip(bars, top_groups.values)):\n",
    "            plt.text(value + 0.5, i, str(value), va='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/static\", \"top_working_groups.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Interactive sunburst chart for groups by period\n",
    "        group_period = self.individual_df.groupby(['Year_Period', 'Working_Groups_Clean']).size().reset_index(name='Publications')\n",
    "        \n",
    "        fig = px.sunburst(\n",
    "            group_period, \n",
    "            path=['Year_Period', 'Working_Groups_Clean'], \n",
    "            values='Publications',\n",
    "            title='Working Group Activity by Time Period'\n",
    "        )\n",
    "        fig.write_html(os.path.join(self.output_dir, \"02_visualizations/interactive\", \"groups_by_period_sunburst.html\"))\n",
    "    \n",
    "    def create_collaboration_analysis(self):\n",
    "        \"\"\"Create detailed collaboration analysis\"\"\"\n",
    "        \n",
    "        # Collaboration patterns\n",
    "        collab_patterns = self.main_df['Num_Collaborating_Groups'].value_counts().sort_index()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(collab_patterns.index, collab_patterns.values, color='lightgreen', alpha=0.8)\n",
    "        plt.xlabel('Number of Collaborating Groups', fontsize=12)\n",
    "        plt.ylabel('Number of Publications', fontsize=12)\n",
    "        plt.title('Distribution of Collaboration Patterns', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/static\", \"collaboration_patterns.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Top collaborating pairs\n",
    "        pair_collabs = Counter()\n",
    "        for _, row in self.collaboration_df.iterrows():\n",
    "            groups = [g.strip().replace('ag_', '') for g in row['Working_Groups'].split(',')]\n",
    "            for i in range(len(groups)):\n",
    "                for j in range(i+1, len(groups)):\n",
    "                    pair = tuple(sorted([groups[i], groups[j]]))\n",
    "                    pair_collabs[pair] += 1\n",
    "        \n",
    "        # Save top pairs\n",
    "        top_pairs = dict(pair_collabs.most_common(20))\n",
    "        top_pairs_df = pd.DataFrame(list(top_pairs.items()), columns=['Group_Pair', 'Collaborations'])\n",
    "        top_pairs_df['Group1'] = top_pairs_df['Group_Pair'].apply(lambda x: x[0])\n",
    "        top_pairs_df['Group2'] = top_pairs_df['Group_Pair'].apply(lambda x: x[1])\n",
    "        top_pairs_df.to_csv(os.path.join(self.output_dir, \"06_collaboration_metrics\", \"top_collaborating_pairs.csv\"), index=False)\n",
    "        \n",
    "        # Collaboration heatmap for top groups\n",
    "        top_15_groups = self.individual_df['Working_Groups_Clean'].value_counts().head(15).index\n",
    "        \n",
    "        # Create collaboration matrix\n",
    "        collab_matrix = pd.DataFrame(0, index=top_15_groups, columns=top_15_groups)\n",
    "        \n",
    "        for _, row in self.collaboration_df.iterrows():\n",
    "            groups = [g.strip().replace('ag_', '') for g in row['Working_Groups'].split(',')]\n",
    "            groups = [g for g in groups if g in top_15_groups]\n",
    "            \n",
    "            for i in range(len(groups)):\n",
    "                for j in range(i+1, len(groups)):\n",
    "                    collab_matrix.loc[groups[i], groups[j]] += 1\n",
    "                    collab_matrix.loc[groups[j], groups[i]] += 1\n",
    "        \n",
    "        plt.figure(figsize=(14, 12))\n",
    "        sns.heatmap(collab_matrix, annot=True, cmap='Blues', fmt='d', square=True)\n",
    "        plt.title('Collaboration Heatmap: Top 15 Working Groups', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Working Groups', fontsize=12)\n",
    "        plt.ylabel('Working Groups', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/static\", \"collaboration_heatmap.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def create_publication_type_analysis(self):\n",
    "        \"\"\"Analyze publication types\"\"\"\n",
    "        \n",
    "        pub_types = self.main_df['Publication_Type'].value_counts()\n",
    "        \n",
    "        # Pie chart\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        colors = sns.color_palette(\"husl\", len(pub_types))\n",
    "        wedges, texts, autotexts = plt.pie(pub_types.values, labels=pub_types.index, autopct='%1.1f%%', \n",
    "                                          colors=colors, startangle=90)\n",
    "        \n",
    "        # Enhance text\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "        \n",
    "        plt.title('Distribution of Publication Types', fontsize=16, fontweight='bold')\n",
    "        plt.axis('equal')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/static\", \"publication_types_pie.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Publication types over time\n",
    "        pub_type_time = pd.crosstab(self.main_df['Publication_Year'], self.main_df['Publication_Type'])\n",
    "        \n",
    "        # Interactive stacked area chart\n",
    "        fig = px.area(\n",
    "            pub_type_time.reset_index(), \n",
    "            x='Publication_Year', \n",
    "            y=pub_type_time.columns.tolist(),\n",
    "            title='Publication Types Evolution Over Time'\n",
    "        )\n",
    "        fig.write_html(os.path.join(self.output_dir, \"02_visualizations/interactive\", \"publication_types_timeline.html\"))\n",
    "    \n",
    "    def create_journal_analysis(self):\n",
    "        \"\"\"Analyze journal patterns and impact\"\"\"\n",
    "        \n",
    "        # Top journals\n",
    "        top_journals = self.main_df['Journal'].value_counts().head(20)\n",
    "        \n",
    "        plt.figure(figsize=(14, 10))\n",
    "        bars = plt.barh(range(len(top_journals)), top_journals.values, color='mediumpurple')\n",
    "        plt.yticks(range(len(top_journals)), top_journals.index)\n",
    "        plt.xlabel('Number of Publications', fontsize=12)\n",
    "        plt.title('Top 20 Journals by Publication Count', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, value) in enumerate(zip(bars, top_journals.values)):\n",
    "            plt.text(value + 0.1, i, str(value), va='center', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/static\", \"top_journals.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Journal diversity over time\n",
    "        yearly_journal_diversity = self.main_df.groupby('Publication_Year')['Journal'].nunique()\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(yearly_journal_diversity.index, yearly_journal_diversity.values, \n",
    "                marker='o', linewidth=3, markersize=8, color='purple')\n",
    "        plt.title('Journal Diversity Over Time', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Year', fontsize=12)\n",
    "        plt.ylabel('Number of Unique Journals', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/static\", \"journal_diversity.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def create_open_access_analysis(self):\n",
    "        \"\"\"Analyze open access patterns\"\"\"\n",
    "        \n",
    "        # Open access rate over time\n",
    "        oa_time = self.main_df.groupby('Publication_Year')['Open_Access_Binary'].mean() * 100\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(oa_time.index, oa_time.values, color='forestgreen', alpha=0.8)\n",
    "        plt.plot(oa_time.index, oa_time.values, color='darkgreen', marker='o', linewidth=2, markersize=6)\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.title('Open Access Rate Over Time', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Year', fontsize=12)\n",
    "        plt.ylabel('Open Access Rate (%)', fontsize=12)\n",
    "        plt.ylim(0, 100)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/static\", \"open_access_trends.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Open access by publication type\n",
    "        oa_by_type = self.main_df.groupby('Publication_Type')['Open_Access_Binary'].agg(['count', 'sum', 'mean']).round(3)\n",
    "        oa_by_type['oa_rate'] = oa_by_type['mean'] * 100\n",
    "        oa_by_type.to_csv(os.path.join(self.output_dir, \"08_impact_analysis\", \"open_access_by_type.csv\"))\n",
    "        \n",
    "        # Interactive stacked bar for OA by groups\n",
    "        oa_groups = self.individual_df.groupby(['Working_Groups_Clean', 'Open_Access']).size().unstack(fill_value=0)\n",
    "        oa_groups['Total'] = oa_groups.sum(axis=1)\n",
    "        oa_groups = oa_groups.sort_values('Total', ascending=False).head(15)\n",
    "        \n",
    "        fig = px.bar(\n",
    "            oa_groups.reset_index(), \n",
    "            x='Working_Groups_Clean', \n",
    "            y=['Yes', 'No'],\n",
    "            title='Open Access Distribution by Top 15 Working Groups',\n",
    "            labels={'value': 'Publications', 'variable': 'Open Access'}\n",
    "        )\n",
    "        fig.update_xaxes(tickangle=45)\n",
    "        fig.write_html(os.path.join(self.output_dir, \"02_visualizations/interactive\", \"open_access_by_groups.html\"))\n",
    "    \n",
    "    def create_productivity_analysis(self):\n",
    "        \"\"\"Analyze productivity patterns and trends\"\"\"\n",
    "        print(\"📊 Creating productivity analysis...\")\n",
    "        \n",
    "        # Group productivity metrics\n",
    "        group_productivity = self.individual_df.groupby('Working_Groups_Clean').agg({\n",
    "            'Publication_Year': ['count', 'min', 'max'],\n",
    "            'Is_Collaboration': 'mean',\n",
    "            'Open_Access_Binary': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        group_productivity.columns = ['Total_Publications', 'First_Year', 'Last_Year', 'Collaboration_Rate', 'Open_Access_Rate']\n",
    "        group_productivity['Active_Years'] = group_productivity['Last_Year'] - group_productivity['First_Year'] + 1\n",
    "        group_productivity['Publications_Per_Year'] = group_productivity['Total_Publications'] / group_productivity['Active_Years']\n",
    "        \n",
    "        group_productivity = group_productivity.sort_values('Total_Publications', ascending=False)\n",
    "        group_productivity.to_csv(os.path.join(self.output_dir, \"07_productivity_analysis\", \"group_productivity_metrics.csv\"))\n",
    "        \n",
    "        # Productivity vs collaboration scatter plot\n",
    "        top_20_groups = group_productivity.head(20)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(top_20_groups['Publications_Per_Year'], top_20_groups['Collaboration_Rate'] * 100,\n",
    "                             s=top_20_groups['Total_Publications'] * 10, alpha=0.6, c='coral')\n",
    "        \n",
    "        # Add labels for top groups\n",
    "        for idx, row in top_20_groups.head(10).iterrows():\n",
    "            plt.annotate(idx, (row['Publications_Per_Year'], row['Collaboration_Rate'] * 100),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        plt.xlabel('Publications per Year', fontsize=12)\n",
    "        plt.ylabel('Collaboration Rate (%)', fontsize=12)\n",
    "        plt.title('Productivity vs Collaboration Rate\\n(Bubble size = Total Publications)', fontsize=16, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"02_visualizations/static\", \"productivity_vs_collaboration.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Author productivity analysis\n",
    "        author_productivity = self.author_df.groupby('Author_Name').agg({\n",
    "            'Publication_Year': ['count', 'min', 'max'],\n",
    "            'Is_First_Author': 'sum',\n",
    "            'Is_Last_Author': 'sum',\n",
    "            'Working_Groups_Clean': 'nunique'\n",
    "        }).round(3)\n",
    "        \n",
    "        author_productivity.columns = ['Total_Publications', 'First_Year', 'Last_Year', 'First_Author_Count', 'Last_Author_Count', 'Working_Groups_Count']\n",
    "        author_productivity['Active_Years'] = author_productivity['Last_Year'] - author_productivity['First_Year'] + 1\n",
    "        author_productivity = author_productivity[author_productivity['Total_Publications'] >= 3].sort_values('Total_Publications', ascending=False)\n",
    "        \n",
    "        author_productivity.to_csv(os.path.join(self.output_dir, \"07_productivity_analysis\", \"author_productivity_metrics.csv\"))\n",
    "    \n",
    "    def create_strategic_insights(self):\n",
    "        \"\"\"Generate strategic insights and recommendations\"\"\"\n",
    "        print(\"🎯 Generating strategic insights...\")\n",
    "        \n",
    "        insights = {\n",
    "            'Research Output Trends': self.analyze_output_trends(),\n",
    "            'Collaboration Effectiveness': self.analyze_collaboration_effectiveness(),\n",
    "            'Impact and Visibility': self.analyze_impact_visibility(),\n",
    "            'Strategic Recommendations': self.generate_recommendations()\n",
    "        }\n",
    "        \n",
    "        # Create comprehensive report\n",
    "        report_html = self.create_executive_report(insights)\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, \"03_reports\", \"executive_summary.html\"), 'w', encoding='utf-8') as f:\n",
    "            f.write(report_html)\n",
    "        \n",
    "        # Save insights as structured data\n",
    "        import json\n",
    "        with open(os.path.join(self.output_dir, \"09_strategic_insights\", \"insights_summary.json\"), 'w') as f:\n",
    "            json.dump(insights, f, indent=2, default=str)\n",
    "    \n",
    "    def analyze_output_trends(self):\n",
    "        \"\"\"Analyze research output trends\"\"\"\n",
    "        current_year = 2024\n",
    "        recent_years = self.main_df[self.main_df['Publication_Year'] >= current_year - 2]\n",
    "        earlier_years = self.main_df[self.main_df['Publication_Year'] < current_year - 2]\n",
    "        \n",
    "        trends = {\n",
    "            'total_publications': len(self.main_df),\n",
    "            'recent_output': len(recent_years),\n",
    "            'earlier_output': len(earlier_years),\n",
    "            'growth_rate': (len(recent_years) / max(len(earlier_years), 1) - 1) * 100,\n",
    "            'peak_year': int(self.main_df['Publication_Year'].value_counts().idxmax()),\n",
    "            'peak_year_count': int(self.main_df['Publication_Year'].value_counts().max()),\n",
    "            'avg_publications_per_year': len(self.main_df) / (current_year - 2014 + 1),\n",
    "            'most_productive_groups': dict(self.individual_df['Working_Groups_Clean'].value_counts().head(5)),\n",
    "            'emerging_groups': self.identify_emerging_groups()\n",
    "        }\n",
    "        \n",
    "        return trends\n",
    "    \n",
    "    def identify_emerging_groups(self):\n",
    "        \"\"\"Identify groups with increasing publication trends\"\"\"\n",
    "        recent_groups = self.individual_df[self.individual_df['Publication_Year'] >= 2022]['Working_Groups_Clean'].value_counts()\n",
    "        earlier_groups = self.individual_df[self.individual_df['Publication_Year'] < 2022]['Working_Groups_Clean'].value_counts()\n",
    "        \n",
    "        emerging = {}\n",
    "        for group in recent_groups.index:\n",
    "            recent_count = recent_groups.get(group, 0)\n",
    "            earlier_count = earlier_groups.get(group, 0)\n",
    "            if recent_count > 0 and earlier_count > 0:\n",
    "                growth = (recent_count / earlier_count - 1) * 100\n",
    "                if growth > 50:  # 50% growth threshold\n",
    "                    emerging[group] = round(growth, 1)\n",
    "        \n",
    "        return dict(sorted(emerging.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "    \n",
    "    def analyze_collaboration_effectiveness(self):\n",
    "        \"\"\"Analyze collaboration patterns and effectiveness\"\"\"\n",
    "        \n",
    "        collab_stats = {\n",
    "            'collaboration_rate': round(self.main_df['Is_Collaboration'].mean() * 100, 1),\n",
    "            'avg_collaborators_per_publication': round(self.main_df['Num_Collaborating_Groups'].mean(), 2),\n",
    "            'max_collaboration_size': int(self.main_df['Num_Collaborating_Groups'].max()),\n",
    "            'most_collaborative_groups': dict(\n",
    "                self.individual_df[self.individual_df['Is_Collaboration'] == True]['Working_Groups_Clean'].value_counts().head(5)\n",
    "            ),\n",
    "            'top_collaboration_pairs': self.get_top_collaboration_pairs(),\n",
    "            'cross_project_collaborations': self.count_cross_project_collaborations(),\n",
    "            'collaboration_trends': self.analyze_collaboration_trends()\n",
    "        }\n",
    "        \n",
    "        return collab_stats\n",
    "    \n",
    "    def get_top_collaboration_pairs(self):\n",
    "        \"\"\"Get top collaborating pairs\"\"\"\n",
    "        pair_collabs = Counter()\n",
    "        for _, row in self.collaboration_df.iterrows():\n",
    "            groups = [g.strip().replace('ag_', '') for g in row['Working_Groups'].split(',')]\n",
    "            for i in range(len(groups)):\n",
    "                for j in range(i+1, len(groups)):\n",
    "                    pair = f\"{groups[i]} + {groups[j]}\"\n",
    "                    pair_collabs[pair] += 1\n",
    "        \n",
    "        return dict(pair_collabs.most_common(5))\n",
    "    \n",
    "    def count_cross_project_collaborations(self):\n",
    "        \"\"\"Count collaborations across different subprojects\"\"\"\n",
    "        cross_project = 0\n",
    "        for _, row in self.collaboration_df.iterrows():\n",
    "            if pd.notna(row['Subproject']) and ',' in row['Subproject']:\n",
    "                projects = [p.strip() for p in row['Subproject'].split(',')]\n",
    "                unique_projects = set([p[0] for p in projects if len(p) > 0])  # First letter (A, B, C, etc.)\n",
    "                if len(unique_projects) > 1:\n",
    "                    cross_project += 1\n",
    "        \n",
    "        return cross_project\n",
    "    \n",
    "    def analyze_collaboration_trends(self):\n",
    "        \"\"\"Analyze how collaboration has evolved\"\"\"\n",
    "        yearly_collab = self.main_df.groupby('Publication_Year')['Is_Collaboration'].mean() * 100\n",
    "        \n",
    "        return {\n",
    "            'early_period_rate': round(yearly_collab[yearly_collab.index <= 2018].mean(), 1),\n",
    "            'recent_period_rate': round(yearly_collab[yearly_collab.index >= 2019].mean(), 1),\n",
    "            'trend_direction': 'increasing' if yearly_collab.iloc[-1] > yearly_collab.iloc[0] else 'decreasing'\n",
    "        }\n",
    "    \n",
    "    def analyze_impact_visibility(self):\n",
    "        \"\"\"Analyze impact and visibility metrics\"\"\"\n",
    "        \n",
    "        impact_stats = {\n",
    "            'open_access_rate': round(self.main_df['Open_Access_Binary'].mean() * 100, 1),\n",
    "            'journal_diversity': self.main_df['Journal'].nunique(),\n",
    "            'top_journals': dict(self.main_df['Journal'].value_counts().head(5)),\n",
    "            'publication_types': dict(self.main_df['Publication_Type'].value_counts()),\n",
    "            'international_visibility': self.assess_international_visibility(),\n",
    "            'research_field_diversity': self.assess_field_diversity()\n",
    "        }\n",
    "        \n",
    "        return impact_stats\n",
    "    \n",
    "    def assess_international_visibility(self):\n",
    "        \"\"\"Assess international research visibility\"\"\"\n",
    "        # This is a simplified assessment based on journal names and patterns\n",
    "        international_journals = ['Cell', 'Nature', 'Science', 'PNAS', 'Journal', 'European', 'International']\n",
    "        \n",
    "        international_pubs = 0\n",
    "        for journal in self.main_df['Journal']:\n",
    "            if any(keyword in str(journal) for keyword in international_journals):\n",
    "                international_pubs += 1\n",
    "        \n",
    "        return {\n",
    "            'international_publications': international_pubs,\n",
    "            'international_rate': round(international_pubs / len(self.main_df) * 100, 1)\n",
    "        }\n",
    "    \n",
    "    def assess_field_diversity(self):\n",
    "        \"\"\"Assess research field diversity based on journal patterns\"\"\"\n",
    "        # Simplified field classification based on journal keywords\n",
    "        field_keywords = {\n",
    "            'Cardiology': ['cardio', 'heart', 'cardiac'],\n",
    "            'Cell Biology': ['cell', 'molecular', 'biology'],\n",
    "            'Physiology': ['physiol', 'function'],\n",
    "            'Biochemistry': ['biochem', 'protein', 'enzyme'],\n",
    "            'Medical': ['medical', 'clinical', 'medicine']\n",
    "        }\n",
    "        \n",
    "        field_counts = defaultdict(int)\n",
    "        for journal in self.main_df['Journal']:\n",
    "            journal_lower = str(journal).lower()\n",
    "            for field, keywords in field_keywords.items():\n",
    "                if any(keyword in journal_lower for keyword in keywords):\n",
    "                    field_counts[field] += 1\n",
    "                    break\n",
    "            else:\n",
    "                field_counts['Other'] += 1\n",
    "        \n",
    "        return dict(field_counts)\n",
    "    \n",
    "    def generate_recommendations(self):\n",
    "        \"\"\"Generate strategic recommendations\"\"\"\n",
    "        \n",
    "        recommendations = {\n",
    "            'Collaboration Enhancement': [\n",
    "                \"Consider facilitating more cross-project (A-B-C) collaborations to increase innovation\",\n",
    "                \"Organize regular collaboration workshops for underconnected groups\",\n",
    "                \"Create incentives for multi-group publications\"\n",
    "            ],\n",
    "            'Research Output Optimization': [\n",
    "                \"Support emerging high-growth working groups with additional resources\",\n",
    "                \"Encourage consistent publication output from all groups\",\n",
    "                \"Consider establishing publication targets and support mechanisms\"\n",
    "            ],\n",
    "            'Visibility and Impact': [\n",
    "                f\"Increase open access rate from current {self.main_df['Open_Access_Binary'].mean() * 100:.1f}% to >80%\",\n",
    "                \"Target more high-impact international journals\",\n",
    "                \"Develop strategic communication plan for research dissemination\"\n",
    "            ],\n",
    "            'Network Strengthening': [\n",
    "                \"Identify and support key connector groups in collaboration networks\",\n",
    "                \"Foster new collaboration pathways between isolated groups\",\n",
    "                \"Create mentorship programs pairing high-output with emerging groups\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def create_executive_report(self, insights):\n",
    "        \"\"\"Create comprehensive HTML executive report\"\"\"\n",
    "        \n",
    "        html_template = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>SFB1002 Publication Analysis - Executive Summary</title>\n",
    "            <style>\n",
    "                body {{ font-family: 'Segoe UI', Arial, sans-serif; margin: 40px; line-height: 1.6; color: #333; }}\n",
    "                .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; }}\n",
    "                .section {{ margin: 30px 0; padding: 20px; border-left: 4px solid #667eea; background: #f8f9fa; border-radius: 5px; }}\n",
    "                .metric {{ display: inline-block; margin: 10px 20px 10px 0; padding: 15px; background: white; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n",
    "                .metric-value {{ font-size: 2em; font-weight: bold; color: #667eea; }}\n",
    "                .metric-label {{ font-size: 0.9em; color: #666; }}\n",
    "                .recommendation {{ background: #e8f5e8; padding: 15px; margin: 10px 0; border-radius: 5px; border-left: 4px solid #28a745; }}\n",
    "                ul {{ margin: 15px 0; }}\n",
    "                li {{ margin: 5px 0; }}\n",
    "                .top-list {{ background: white; padding: 15px; border-radius: 5px; margin: 10px 0; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"header\">\n",
    "                <h1>SFB1002 Publication Analysis</h1>\n",
    "                <h2>Executive Summary & Strategic Insights</h2>\n",
    "                <p>Analysis Period: 2014-2024 | Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>📊 Key Performance Metrics</h2>\n",
    "                <div class=\"metric\">\n",
    "                    <div class=\"metric-value\">{insights['Research Output Trends']['total_publications']}</div>\n",
    "                    <div class=\"metric-label\">Total Publications</div>\n",
    "                </div>\n",
    "                <div class=\"metric\">\n",
    "                    <div class=\"metric-value\">{len(set(self.individual_df['Working_Groups_Clean']))}</div>\n",
    "                    <div class=\"metric-label\">Active Working Groups</div>\n",
    "                </div>\n",
    "                <div class=\"metric\">\n",
    "                    <div class=\"metric-value\">{insights['Collaboration Effectiveness']['collaboration_rate']}%</div>\n",
    "                    <div class=\"metric-label\">Collaboration Rate</div>\n",
    "                </div>\n",
    "                <div class=\"metric\">\n",
    "                    <div class=\"metric-value\">{insights['Impact and Visibility']['open_access_rate']}%</div>\n",
    "                    <div class=\"metric-label\">Open Access Rate</div>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>📈 Research Output Trends</h2>\n",
    "                <p><strong>Peak Performance:</strong> {insights['Research Output Trends']['peak_year']} with {insights['Research Output Trends']['peak_year_count']} publications</p>\n",
    "                <p><strong>Average Output:</strong> {insights['Research Output Trends']['avg_publications_per_year']:.1f} publications per year</p>\n",
    "                \n",
    "                <div class=\"top-list\">\n",
    "                    <h3>Most Productive Working Groups:</h3>\n",
    "                    <ul>\n",
    "                        {''.join([f\"<li>{group}: {count} publications</li>\" for group, count in insights['Research Output Trends']['most_productive_groups'].items()])}\n",
    "                    </ul>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"top-list\">\n",
    "                    <h3>Emerging High-Growth Groups:</h3>\n",
    "                    <ul>\n",
    "                        {''.join([f\"<li>{group}: +{growth}% growth</li>\" for group, growth in insights['Research Output Trends']['emerging_groups'].items()])}\n",
    "                    </ul>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>🤝 Collaboration Analysis</h2>\n",
    "                <p><strong>Collaboration Effectiveness:</strong> {insights['Collaboration Effectiveness']['collaboration_rate']}% of publications involve multiple groups</p>\n",
    "                <p><strong>Average Collaborators:</strong> {insights['Collaboration Effectiveness']['avg_collaborators_per_publication']} groups per publication</p>\n",
    "                <p><strong>Cross-Project Collaborations:</strong> {insights['Collaboration Effectiveness']['cross_project_collaborations']} publications span multiple subprojects</p>\n",
    "                \n",
    "                <div class=\"top-list\">\n",
    "                    <h3>Top Collaboration Pairs:</h3>\n",
    "                    <ul>\n",
    "                        {''.join([f\"<li>{pair}: {count} joint publications</li>\" for pair, count in insights['Collaboration Effectiveness']['top_collaboration_pairs'].items()])}\n",
    "                    </ul>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>🎯 Impact & Visibility</h2>\n",
    "                <p><strong>Journal Diversity:</strong> {insights['Impact and Visibility']['journal_diversity']} unique journals</p>\n",
    "                <p><strong>International Visibility:</strong> {insights['Impact and Visibility']['international_visibility']['international_rate']}% publications in international journals</p>\n",
    "                \n",
    "                <div class=\"top-list\">\n",
    "                    <h3>Top Publication Venues:</h3>\n",
    "                    <ul>\n",
    "                        {''.join([f\"<li>{journal}: {count} publications</li>\" for journal, count in insights['Impact and Visibility']['top_journals'].items()])}\n",
    "                    </ul>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>💡 Strategic Recommendations</h2>\n",
    "                {self.format_recommendations(insights['Strategic Recommendations'])}\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>📁 Generated Outputs</h2>\n",
    "                <p>This analysis has generated comprehensive outputs organized in:</p>\n",
    "                <ul>\n",
    "                    <li><strong>Cleaned Data:</strong> 5 specialized datasets for different analysis perspectives</li>\n",
    "                    <li><strong>Visualizations:</strong> 15+ static and interactive charts, plus network diagrams</li>\n",
    "                    <li><strong>Network Analysis:</strong> Collaboration networks with centrality metrics</li>\n",
    "                    <li><strong>Temporal Analysis:</strong> Evolution of collaboration patterns over time</li>\n",
    "                    <li><strong>Productivity Metrics:</strong> Group and author performance indicators</li>\n",
    "                    <li><strong>Strategic Insights:</strong> Data-driven recommendations for future planning</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        return html_template\n",
    "    \n",
    "    def format_recommendations(self, recommendations):\n",
    "        \"\"\"Format recommendations for HTML report\"\"\"\n",
    "        html = \"\"\n",
    "        for category, rec_list in recommendations.items():\n",
    "            html += f\"<h3>{category}</h3>\"\n",
    "            for rec in rec_list:\n",
    "                html += f'<div class=\"recommendation\">{rec}</div>'\n",
    "        return html\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Run the complete analysis pipeline\"\"\"\n",
    "        print(\"🚀 Starting SFB1002 Complete Publication Analysis...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Load and clean data\n",
    "        self.load_and_clean_data()\n",
    "        \n",
    "        # Step 2: Generate basic statistics\n",
    "        stats = self.generate_basic_statistics()\n",
    "        \n",
    "        # Step 3: Temporal analysis\n",
    "        self.create_temporal_analysis()\n",
    "        \n",
    "        # Step 4: Network analysis\n",
    "        self.create_collaboration_networks()\n",
    "        \n",
    "        # Step 5: Comprehensive visualizations\n",
    "        self.create_comprehensive_visualizations()\n",
    "        \n",
    "        # Step 6: Productivity analysis\n",
    "        self.create_productivity_analysis()\n",
    "        \n",
    "        # Step 7: Strategic insights\n",
    "        self.create_strategic_insights()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"✅ Analysis Complete!\")\n",
    "        print(f\"📁 All outputs saved to: {self.output_dir}\")\n",
    "        print(\"\\n📋 Summary:\")\n",
    "        print(f\"   • {stats['Overview']['Total Publications']} publications analyzed\")\n",
    "        print(f\"   • {stats['Overview']['Unique Working Groups']} working groups\")\n",
    "        print(f\"   • {stats['Overview']['Unique Authors']} unique authors\")\n",
    "        print(f\"   • {stats['Overview']['Collaboration Rate']} collaboration rate\")\n",
    "        print(f\"   • 10 organized output folders with 50+ analysis files\")\n",
    "        print(\"\\n🎯 Key files to review:\")\n",
    "        print(\"   • 03_reports/executive_summary.html\")\n",
    "        print(\"   • 02_visualizations/interactive/ (for detailed exploration)\")\n",
    "        print(\"   • 06_collaboration_metrics/ (for network insights)\")\n",
    "\n",
    "# Usage Instructions\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    To run this analysis:\n",
    "    \n",
    "    1. Install required packages:\n",
    "       pip install pandas numpy matplotlib seaborn plotly networkx openpyxl\n",
    "    \n",
    "    2. Update the file path below to your Excel file location\n",
    "    \n",
    "    3. Run the analysis:\n",
    "       python sfb1002_analysis.py\n",
    "    \"\"\"\n",
    "    \n",
    "    # CHANGE THIS PATH TO YOUR EXCEL FILE LOCATION\n",
    "    file_path = \"2025-07-01_excel_export.xlsx\"\n",
    "    \n",
    "    # Initialize and run the complete analysis\n",
    "    analyzer = SFB1002Analyzer(file_path)\n",
    "    analyzer.run_complete_analysis()\n",
    "    \n",
    "    print(\"\\n\" + \"🎉\" * 20)\n",
    "    print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"🎉\" * 20)\n",
    "\n",
    "# Additional utility functions for extended analysis\n",
    "\n",
    "def create_advanced_network_metrics(analyzer):\n",
    "    \"\"\"Create advanced network analysis with additional metrics\"\"\"\n",
    "    \n",
    "    # Load the group collaboration network\n",
    "    network_file = os.path.join(analyzer.output_dir, \"02_visualizations/networks\", \"group_network.gexf\")\n",
    "    if os.path.exists(network_file):\n",
    "        G = nx.read_gexf(network_file)\n",
    "        \n",
    "        # Advanced centrality metrics\n",
    "        advanced_metrics = {\n",
    "            'clustering': nx.clustering(G),\n",
    "            'pagerank': nx.pagerank(G),\n",
    "            'katz_centrality': nx.katz_centrality(G, max_iter=1000),\n",
    "            'harmonic_centrality': nx.harmonic_centrality(G),\n",
    "            'load_centrality': nx.load_centrality(G)\n",
    "        }\n",
    "        \n",
    "        # Network-level metrics\n",
    "        network_stats = {\n",
    "            'density': nx.density(G),\n",
    "            'transitivity': nx.transitivity(G),\n",
    "            'average_clustering': nx.average_clustering(G),\n",
    "            'number_of_nodes': G.number_of_nodes(),\n",
    "            'number_of_edges': G.number_of_edges(),\n",
    "            'average_degree': sum(dict(G.degree()).values()) / G.number_of_nodes(),\n",
    "            'diameter': nx.diameter(G) if nx.is_connected(G) else 'Graph not connected',\n",
    "            'radius': nx.radius(G) if nx.is_connected(G) else 'Graph not connected'\n",
    "        }\n",
    "        \n",
    "        # Save advanced metrics\n",
    "        advanced_df = pd.DataFrame(advanced_metrics)\n",
    "        advanced_df.to_csv(os.path.join(analyzer.output_dir, \"06_collaboration_metrics\", \"advanced_centrality_metrics.csv\"))\n",
    "        \n",
    "        # Save network statistics\n",
    "        with open(os.path.join(analyzer.output_dir, \"06_collaboration_metrics\", \"network_statistics.txt\"), 'w') as f:\n",
    "            f.write(\"NETWORK-LEVEL STATISTICS\\n\")\n",
    "            f.write(\"=\" * 30 + \"\\n\\n\")\n",
    "            for metric, value in network_stats.items():\n",
    "                f.write(f\"{metric.replace('_', ' ').title()}: {value}\\n\")\n",
    "\n",
    "def create_longitudinal_collaboration_analysis(analyzer):\n",
    "    \"\"\"Create detailed longitudinal analysis of collaboration patterns\"\"\"\n",
    "    \n",
    "    print(\"📊 Creating longitudinal collaboration analysis...\")\n",
    "    \n",
    "    # Analyze collaboration strength over time\n",
    "    yearly_collaboration_strength = {}\n",
    "    \n",
    "    for year in range(2014, 2025):\n",
    "        year_data = analyzer.main_df[analyzer.main_df['Publication_Year'] == year]\n",
    "        year_collabs = year_data[year_data['Is_Collaboration'] == True]\n",
    "        \n",
    "        # Calculate collaboration metrics for this year\n",
    "        if len(year_collabs) > 0:\n",
    "            avg_collaborators = year_collabs['Num_Collaborating_Groups'].mean()\n",
    "            collaboration_rate = len(year_collabs) / len(year_data) * 100\n",
    "            unique_collaborating_groups = set()\n",
    "            \n",
    "            for _, row in year_collabs.iterrows():\n",
    "                groups = [g.strip() for g in row['Working_Groups'].split(',')]\n",
    "                unique_collaborating_groups.update(groups)\n",
    "            \n",
    "            yearly_collaboration_strength[year] = {\n",
    "                'collaboration_rate': collaboration_rate,\n",
    "                'avg_collaborators_per_pub': avg_collaborators,\n",
    "                'unique_collaborating_groups': len(unique_collaborating_groups),\n",
    "                'total_publications': len(year_data),\n",
    "                'collaborative_publications': len(year_collabs)\n",
    "            }\n",
    "    \n",
    "    # Save longitudinal data\n",
    "    longitudinal_df = pd.DataFrame(yearly_collaboration_strength).T\n",
    "    longitudinal_df.to_csv(os.path.join(analyzer.output_dir, \"05_temporal_analysis\", \"longitudinal_collaboration_metrics.csv\"))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Collaboration rate over time\n",
    "    axes[0,0].plot(longitudinal_df.index, longitudinal_df['collaboration_rate'], marker='o', linewidth=2, markersize=6)\n",
    "    axes[0,0].set_title('Collaboration Rate Over Time')\n",
    "    axes[0,0].set_ylabel('Collaboration Rate (%)')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Average collaborators per publication\n",
    "    axes[0,1].plot(longitudinal_df.index, longitudinal_df['avg_collaborators_per_pub'], marker='s', linewidth=2, markersize=6, color='orange')\n",
    "    axes[0,1].set_title('Average Collaborators per Publication')\n",
    "    axes[0,1].set_ylabel('Average Collaborators')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Number of collaborating groups\n",
    "    axes[1,0].bar(longitudinal_df.index, longitudinal_df['unique_collaborating_groups'], alpha=0.7, color='green')\n",
    "    axes[1,0].set_title('Unique Collaborating Groups per Year')\n",
    "    axes[1,0].set_ylabel('Number of Groups')\n",
    "    axes[1,0].set_xlabel('Year')\n",
    "    \n",
    "    # Plot 4: Total vs Collaborative publications\n",
    "    axes[1,1].bar(longitudinal_df.index, longitudinal_df['total_publications'], alpha=0.7, label='Total Publications', color='lightblue')\n",
    "    axes[1,1].bar(longitudinal_df.index, longitudinal_df['collaborative_publications'], alpha=0.9, label='Collaborative Publications', color='darkblue')\n",
    "    axes[1,1].set_title('Publications: Total vs Collaborative')\n",
    "    axes[1,1].set_ylabel('Number of Publications')\n",
    "    axes[1,1].set_xlabel('Year')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(analyzer.output_dir, \"02_visualizations/static\", \"longitudinal_collaboration_analysis.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_author_influence_network(analyzer):\n",
    "    \"\"\"Create detailed author influence and collaboration network\"\"\"\n",
    "    \n",
    "    print(\"👤 Creating author influence network...\")\n",
    "    \n",
    "    # Focus on authors with significant contributions\n",
    "    author_stats = analyzer.author_df.groupby('Author_Name').agg({\n",
    "        'Publication_Year': ['count', 'min', 'max'],\n",
    "        'Is_First_Author': 'sum',\n",
    "        'Is_Last_Author': 'sum',\n",
    "        'Working_Groups_Clean': lambda x: list(set(x))\n",
    "    })\n",
    "    \n",
    "    author_stats.columns = ['Total_Pubs', 'First_Year', 'Last_Year', 'First_Author_Count', 'Last_Author_Count', 'Working_Groups']\n",
    "    author_stats['Influence_Score'] = (author_stats['First_Author_Count'] * 2 + \n",
    "                                     author_stats['Last_Author_Count'] * 3 + \n",
    "                                     author_stats['Total_Pubs'])\n",
    "    \n",
    "    # Select influential authors (top 30% by influence score)\n",
    "    threshold = author_stats['Influence_Score'].quantile(0.7)\n",
    "    influential_authors = author_stats[author_stats['Influence_Score'] >= threshold]\n",
    "    \n",
    "    # Create author collaboration network\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    for author, stats in influential_authors.iterrows():\n",
    "        G.add_node(author, \n",
    "                  total_pubs=stats['Total_Pubs'],\n",
    "                  influence_score=stats['Influence_Score'],\n",
    "                  working_groups=len(stats['Working_Groups']))\n",
    "    \n",
    "    # Add collaboration edges\n",
    "    author_publications = defaultdict(list)\n",
    "    for _, row in analyzer.author_df.iterrows():\n",
    "        if row['Author_Name'] in influential_authors.index:\n",
    "            pub_id = f\"{row['DOI']}_{row['Publication_Year']}\"\n",
    "            author_publications[pub_id].append(row['Author_Name'])\n",
    "    \n",
    "    for pub_id, authors in author_publications.items():\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i+1, len(authors)):\n",
    "                if G.has_edge(authors[i], authors[j]):\n",
    "                    G[authors[i]][authors[j]]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(authors[i], authors[j], weight=1)\n",
    "    \n",
    "    # Calculate centrality metrics\n",
    "    if len(G.nodes()) > 0:\n",
    "        author_centrality = {\n",
    "            'betweenness': nx.betweenness_centrality(G),\n",
    "            'closeness': nx.closeness_centrality(G),\n",
    "            'degree': nx.degree_centrality(G),\n",
    "            'eigenvector': nx.eigenvector_centrality(G, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        # Combine with influence scores\n",
    "        influence_analysis = pd.DataFrame(author_centrality)\n",
    "        influence_analysis['Influence_Score'] = [influential_authors.loc[author, 'Influence_Score'] for author in influence_analysis.index]\n",
    "        influence_analysis['Total_Publications'] = [influential_authors.loc[author, 'Total_Pubs'] for author in influence_analysis.index]\n",
    "        \n",
    "        influence_analysis.to_csv(os.path.join(analyzer.output_dir, \"06_collaboration_metrics\", \"author_influence_analysis.csv\"))\n",
    "        \n",
    "        # Visualize author network\n",
    "        plt.figure(figsize=(20, 16))\n",
    "        pos = nx.spring_layout(G, k=3, iterations=100)\n",
    "        \n",
    "        # Node sizes based on influence score\n",
    "        node_sizes = [G.nodes[node]['influence_score'] * 20 for node in G.nodes()]\n",
    "        \n",
    "        # Node colors based on number of working groups\n",
    "        node_colors = [G.nodes[node]['working_groups'] for node in G.nodes()]\n",
    "        \n",
    "        # Edge weights\n",
    "        edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "        \n",
    "        # Draw network\n",
    "        nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, \n",
    "                                     cmap='viridis', alpha=0.7)\n",
    "        nx.draw_networkx_edges(G, pos, width=[w*0.5 for w in edge_weights], alpha=0.4)\n",
    "        \n",
    "        # Add labels for top authors only\n",
    "        top_authors = influence_analysis.nlargest(15, 'Influence_Score').index\n",
    "        labels = {author: author.split()[-1] if author in top_authors else '' for author in G.nodes()}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(nodes, label='Number of Working Groups')\n",
    "        \n",
    "        plt.title('Author Influence Network\\n(Node size = Influence Score, Color = Working Group Diversity)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(analyzer.output_dir, \"02_visualizations/networks\", \"author_influence_network.png\"), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save network\n",
    "        nx.write_gexf(G, os.path.join(analyzer.output_dir, \"02_visualizations/networks\", \"author_influence_network.gexf\"))\n",
    "\n",
    "def create_publication_impact_analysis(analyzer):\n",
    "    \"\"\"Analyze publication impact patterns\"\"\"\n",
    "    \n",
    "    print(\"📈 Creating publication impact analysis...\")\n",
    "    \n",
    "    # Journal impact classification (simplified)\n",
    "    high_impact_keywords = ['nature', 'science', 'cell', 'lancet', 'nejm', 'pnas']\n",
    "    \n",
    "    analyzer.main_df['Journal_Impact_Category'] = 'Standard'\n",
    "    for idx, journal in enumerate(analyzer.main_df['Journal']):\n",
    "        journal_lower = str(journal).lower()\n",
    "        if any(keyword in journal_lower for keyword in high_impact_keywords):\n",
    "            analyzer.main_df.iloc[idx, analyzer.main_df.columns.get_loc('Journal_Impact_Category')] = 'High Impact'\n",
    "    \n",
    "    # Impact analysis by groups\n",
    "    impact_by_group = analyzer.individual_df.groupby('Working_Groups_Clean').agg({\n",
    "        'Journal_Impact_Category': lambda x: (x == 'High Impact').sum(),\n",
    "        'Open_Access_Binary': 'mean',\n",
    "        'Publication_Year': 'count'\n",
    "    })\n",
    "    \n",
    "    impact_by_group.columns = ['High_Impact_Count', 'Open_Access_Rate', 'Total_Publications']\n",
    "    impact_by_group['High_Impact_Rate'] = impact_by_group['High_Impact_Count'] / impact_by_group['Total_Publications'] * 100\n",
    "    impact_by_group = impact_by_group.sort_values('High_Impact_Rate', ascending=False)\n",
    "    \n",
    "    impact_by_group.to_csv(os.path.join(analyzer.output_dir, \"08_impact_analysis\", \"impact_analysis_by_group.csv\"))\n",
    "    \n",
    "    # Create impact visualization\n",
    "    top_impact_groups = impact_by_group.head(15)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # High impact rate\n",
    "    bars1 = ax1.barh(range(len(top_impact_groups)), top_impact_groups['High_Impact_Rate'], color='gold')\n",
    "    ax1.set_yticks(range(len(top_impact_groups)))\n",
    "    ax1.set_yticklabels(top_impact_groups.index)\n",
    "    ax1.set_xlabel('High Impact Publication Rate (%)')\n",
    "    ax1.set_title('High Impact Publication Rate by Working Group')\n",
    "    \n",
    "    # Open access rate\n",
    "    bars2 = ax2.barh(range(len(top_impact_groups)), top_impact_groups['Open_Access_Rate'] * 100, color='lightgreen')\n",
    "    ax2.set_yticks(range(len(top_impact_groups)))\n",
    "    ax2.set_yticklabels(top_impact_groups.index)\n",
    "    ax2.set_xlabel('Open Access Rate (%)')\n",
    "    ax2.set_title('Open Access Rate by Working Group')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(analyzer.output_dir, \"02_visualizations/static\", \"impact_analysis_by_group.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def generate_final_summary_report(analyzer):\n",
    "    \"\"\"Generate a comprehensive final summary with all key insights\"\"\"\n",
    "    \n",
    "    print(\"📋 Generating final comprehensive summary...\")\n",
    "    \n",
    "    # Compile all key statistics\n",
    "    summary_stats = {\n",
    "        'Data Overview': {\n",
    "            'Total Publications': len(analyzer.main_df),\n",
    "            'Time Period': f\"{analyzer.main_df['Publication_Year'].min()}-{analyzer.main_df['Publication_Year'].max()}\",\n",
    "            'Unique Working Groups': analyzer.individual_df['Working_Groups_Clean'].nunique(),\n",
    "            'Unique Authors': analyzer.author_df['Author_Name'].nunique(),\n",
    "            'Unique Journals': analyzer.main_df['Journal'].nunique(),\n",
    "        },\n",
    "        \n",
    "        'Collaboration Metrics': {\n",
    "            'Overall Collaboration Rate': f\"{analyzer.main_df['Is_Collaboration'].mean() * 100:.1f}%\",\n",
    "            'Average Collaborators per Publication': f\"{analyzer.main_df['Num_Collaborating_Groups'].mean():.2f}\",\n",
    "            'Maximum Collaboration Size': analyzer.main_df['Num_Collaborating_Groups'].max(),\n",
    "            'Most Collaborative Groups': dict(analyzer.individual_df[analyzer.individual_df['Is_Collaboration'] == True]['Working_Groups_Clean'].value_counts().head(5))\n",
    "        },\n",
    "        \n",
    "        'Productivity Leaders': {\n",
    "            'Most Productive Groups': dict(analyzer.individual_df['Working_Groups_Clean'].value_counts().head(10)),\n",
    "            'Most Prolific Authors': dict(analyzer.author_df['Author_Name'].value_counts().head(10)),\n",
    "            'Peak Publication Year': analyzer.main_df['Publication_Year'].value_counts().idxmax(),\n",
    "            'Peak Year Count': analyzer.main_df['Publication_Year'].value_counts().max()\n",
    "        },\n",
    "        \n",
    "        'Quality & Impact Indicators': {\n",
    "            'Open Access Rate': f\"{analyzer.main_df['Open_Access_Binary'].mean() * 100:.1f}%\",\n",
    "            'Journal Article Rate': f\"{(analyzer.main_df['Publication_Type'] == 'Journal Article').mean() * 100:.1f}%\",\n",
    "            'Review Article Rate': f\"{(analyzer.main_df['Publication_Type'] == 'Review').mean() * 100:.1f}%\",\n",
    "            'Top Publishing Journals': dict(analyzer.main_df['Journal'].value_counts().head(5))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create final summary file\n",
    "    summary_path = os.path.join(analyzer.output_dir, \"09_strategic_insights\", \"comprehensive_summary.txt\")\n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"SFB1002 PUBLICATION ANALYSIS - COMPREHENSIVE SUMMARY\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Analysis Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Data Source: 2025-07-01_excel_export.xlsx\\n\\n\")\n",
    "        \n",
    "        for section, data in summary_stats.items():\n",
    "            f.write(f\"\\n{section.upper()}\\n\")\n",
    "            f.write(\"-\" * len(section) + \"\\n\")\n",
    "            \n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, dict):\n",
    "                    f.write(f\"\\n{key}:\\n\")\n",
    "                    for k, v in value.items():\n",
    "                        f.write(f\"  • {k}: {v}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        f.write(f\"\\n\\nANALYSIS OUTPUTS GENERATED\\n\")\n",
    "        f.write(\"=\" * 30 + \"\\n\")\n",
    "        f.write(\"• 5 cleaned datasets in 01_cleaned_data/\\n\")\n",
    "        f.write(\"• 20+ visualizations in 02_visualizations/\\n\")\n",
    "        f.write(\"• Network analysis files in 02_visualizations/networks/\\n\")\n",
    "        f.write(\"• Executive summary in 03_reports/\\n\")\n",
    "        f.write(\"• Statistical summaries in 04_statistics/\\n\")\n",
    "        f.write(\"• Temporal analysis in 05_temporal_analysis/\\n\")\n",
    "        f.write(\"• Collaboration metrics in 06_collaboration_metrics/\\n\")\n",
    "        f.write(\"• Productivity analysis in 07_productivity_analysis/\\n\")\n",
    "        f.write(\"• Impact analysis in 08_impact_analysis/\\n\")\n",
    "        f.write(\"• Strategic insights in 09_strategic_insights/\\n\")\n",
    "\n",
    "# Extended analysis runner\n",
    "def run_extended_analysis():\n",
    "    \"\"\"Run the extended analysis with additional features\"\"\"\n",
    "    \n",
    "    file_path = \"2025-07-01_excel_export.xlsx\"\n",
    "    \n",
    "    # Run main analysis\n",
    "    analyzer = SFB1002Analyzer(file_path)\n",
    "    analyzer.run_complete_analysis()\n",
    "    \n",
    "    # Run extended analyses\n",
    "    print(\"\\n🔬 Running Extended Analysis Components...\")\n",
    "    \n",
    "    create_advanced_network_metrics(analyzer)\n",
    "    create_longitudinal_collaboration_analysis(analyzer)\n",
    "    create_author_influence_network(analyzer)\n",
    "    create_publication_impact_analysis(analyzer)\n",
    "    generate_final_summary_report(analyzer)\n",
    "    \n",
    "    print(\"\\n\" + \"🎊\" * 25)\n",
    "    print(\"COMPLETE EXTENDED ANALYSIS FINISHED!\")\n",
    "    print(\"🎊\" * 25)\n",
    "    print(f\"\\n📂 Check your results in: {analyzer.output_dir}\")\n",
    "    print(\"\\n🏆 ANALYSIS HIGHLIGHTS:\")\n",
    "    print(\"   • Complete collaboration network analysis\")\n",
    "    print(\"   • Author influence and centrality metrics\") \n",
    "    print(\"   • Longitudinal trend analysis\")\n",
    "    print(\"   • Publication impact assessment\")\n",
    "    print(\"   • Strategic recommendations\")\n",
    "    print(\"   • 60+ output files across 10 categories\")\n",
    "\n",
    "# Uncomment the line below to run the extended analysis\n",
    "# run_extended_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f48a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e994453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caa3f799",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
